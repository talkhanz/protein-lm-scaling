# corresponds to DatasetConfig
dataset:
  dataset_type: "colabfold"
  dataset_loc: "protein_lm/dataset/colabfold/output"
  cluster_loc: "protein_lm/dataset/colabfold/cluster_sizes.csv"
  subsample_size: 6
  split_seed: 2
  val_size: 1
  test_size: 1
  sequence_column_name: "sequence"
  max_sequence_length: 1024
  do_curriculum_learning: false
  curriculum_learning_strategy: sequence_length # supported options - 'sequence_length','ppl' 'plddt'
  curriculum_learning_column_name: sequence_length # supported options - 'sequence_length','ppl' 'plddt'


# corresponds to HuggingFace's TrainingArguments
training_arguments:
  output_dir: "checkpoints/toy"
  max_steps: -1
  num_train_epochs: 1
  learning_rate: 0.1
  weight_decay: 0.1
  save_strategy: "epoch"
  per_device_train_batch_size: 1
  save_steps: 1
  report_to: "none"
  label_names:
    - 'labels'
  no_cuda: false

# corresponds to WandBConfig
wandb:
  name: "toy_hf"
  dir: "wandb_files/"

# corresponds to TokenizerConfig
tokenizer:
  tokenizer_type: "APT"

# corresponds to NNModelConfig
model:
  nn_model_type: "APT"
  nn_model_config_args:
    position_embedding: "learned"
    rope_scaling_factor: 1.0
    rope_theta: 10000
    max_sequence_length: 10
  pretrained_checkpoint: null

# corresponds to DataCollatorConfig
data_collator:
  data_collator_type: "default"
