# corresponds to DatasetConfig
dataset:
  dataset_type: "colabfold"
  dataset_loc: "protein_lm/dataset/colabfold/output"
  cluster_loc: "protein_lm/dataset/colabfold/cluster_sizes.csv"
  subsample_size: 6
  split_seed: 2
  val_size: 2
  test_size: 1
  sequence_column_name: "sequence"
  max_sequence_length: 10
  do_curriculum_learning: false
  curriculum_learning_strategy: sequence_length # supported options - 'sequence_length','ppl' 'plddt'
  curriculum_learning_column_name: sequence_length # supported options - 'sequence_length','ppl' 'plddt'


# corresponds to HuggingFace's TrainingArguments
training_arguments:
  output_dir: "checkpoints/toy"
  max_steps: 10
  num_train_epochs: 10
  learning_rate: 1
  lr_scheduler_type: "linear"
  warmup_steps: 4
  weight_decay: 0.1
  logging_strategy: "steps"
  logging_steps: 1
  save_strategy: "steps"
  evaluation_strategy: "steps"
  per_device_train_batch_size: 1
  save_steps: 1
  report_to: "wandb"
  label_names:
    - 'labels'
  no_cuda: false

# corresponds to WandBConfig
wandb:
  name: "colabfold"
  dir: "experiments/"

# corresponds to TokenizerConfig
tokenizer:
  tokenizer_type: "APT"

# corresponds to NNModelConfig
model:
  nn_model_type: "APT"
  nn_model_config_args:
    position_embedding: "rope"
    rope_scaling_factor: 1.0
    rope_theta: 10000
    max_sequence_length: 10
  pretrained_checkpoint: null

# corresponds to DataCollatorConfig
data_collator:
  data_collator_type: "default"
