# corresponds to DatasetConfig
dataset:
  dataset_type: "multisequence"
  dataset_loc: "protein_lm/dataset/multisequence/uniref_toy_train.csv"
  test_dataset_loc: "protein_lm/dataset/multisequence/uniref_toy_test.csv"
  val_dataset_loc: "protein_lm/dataset/multisequence/uniref_toy_val.csv"
  cluster_loc: ""
  split_seed: 2
  subsample_size: 1.0 # 100% of complete set,this parameter can also be a number
  val_size: 0.1 # 10% of complete set, this parameter can also be a number
  test_size: 0.2 # 20% of complete set, this parameter can also be a number
  index_column_name: "index" # since we will be using an Iterable Dataset with samples spanning multiple workers, we need to
  sequence_column_name: "sequence"
  cond_sequence_column_names: #the order needs to uniref30,50,90 and then alphafold columns as below
  - "UniRef30_clust_str" 
  - "UniRef50_clust_str"
  - "UniRef70_clust_str"
  - "UniRef90_clust_str"
  - "Alphafold_clust_str"
  focus_sequence_column_name: "U100"  
  max_cond_sequence_length: 40
  max_focus_sequence_length: 40
  max_sequence_length: 80 # has to be greater than or equal sum(max_cond_sequence_length,max_focus_sequence_length)
  tokenization_strategy: "otf" #otf = on the fly, tokenization will happen when a sample is returned from the MultiSequenceIterableClass
  



# corresponds to HuggingFace's TrainingArguments
training_arguments:
  output_dir: "checkpoints/toy"
  max_steps: 20
  num_train_epochs: 10
  learning_rate: 0.0003
  lr_scheduler_type: "linear"
  warmup_steps: 4
  weight_decay: 0.1
  logging_strategy: "steps"
  logging_steps: 1
  save_strategy: "steps"
  evaluation_strategy: "steps"
  per_device_train_batch_size: 3
  per_device_eval_batch_size: 3
  save_steps: 1
  eval_steps: 5
  report_to: "none"
  label_names:
    - 'labels'
  no_cuda: false

# corresponds to WandBConfig
wandb:
  name: "seqofseqs"
  dir: "experiments/"

# corresponds to TokenizerConfig
tokenizer:
  tokenizer_type: "APT"
  mask_type: "random_multi_span"
  sequence_mask_fraction:
    condition: 
      uniref30: 0.2
      uniref50: 0.2
      uniref70: 0.2
      uniref90: 0.2
      uniref100: 0.2
      struct: 0.2
    focus: 
      uniref30: 0.0
      uniref50: 0.0
      uniref70: 0.0
      uniref90: 0.0
      uniref100: 0.0
      struct: 0.0
  

# corresponds to NNModelConfig
model:
  nn_model_type: "APT"
  nn_model_config_args:
    position_embedding: "learned"
    position_embedding_type: 'separate_condition_and_focus_position_embeddings'
    rope_scaling_factor: 1.0
    rope_theta: 10000
    max_cond_sequence_length: 40
    max_focus_sequence_length: 40
    max_sequence_length: 80 # has to be greater than or equal sum(max_cond_sequence_length,max_focus_sequence_length)
    attn_type: 'standard'
  pretrained_checkpoint: null

# corresponds to DataCollatorConfig
data_collator:
  data_collator_type: "default"
