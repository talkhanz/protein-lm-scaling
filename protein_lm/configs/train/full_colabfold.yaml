# corresponds to DatasetConfig
dataset:
  dataset_type: "colabfold"
  dataset_loc: "/weka/home-othertea/protein-lm-scaling/colabfold_parsing/output"
  cluster_loc: "/weka/home-othertea/protein-lm-scaling/colabfold_parsing/cluster_sizes.csv"
  subsample_size: 10000
  split_seed: 2
  val_size: 1000
  test_size: 1000
  sequence_column_name: "sequence"
  max_sequence_length: 128
  do_curriculum_learning: false



# corresponds to HuggingFace's TrainingArguments
training_arguments:
  output_dir: "checkpoints/colabfold"
  max_steps: 1000
  num_train_epochs: 1
  learning_rate: 0.1
  lr_scheduler_type: "linear"
  warmup_ratio: 0.1
  weight_decay: 0.1
  logging_strategy: "steps"
  logging_steps: 10
  save_strategy: "steps"
  evaluation_strategy: "steps"
  per_device_train_batch_size: 16
  save_steps: 50
  report_to: "wandb"
  label_names:
    - 'labels'
  no_cuda: false

# corresponds to WandBConfig
wandb:
  name: "colabfold"
  dir: "experiments/"

# corresponds to TokenizerConfig
tokenizer:
  tokenizer_type: "APT"

# corresponds to NNModelConfig
model:
  nn_model_type: "APT"
  nn_model_config_args:
    position_embedding: "rope"
    rope_scaling_factor: 1.0
    rope_theta: 10000
    max_sequence_length: 128
  pretrained_checkpoint: null

# corresponds to DataCollatorConfig
data_collator:
  data_collator_type: "default"
